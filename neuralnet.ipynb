{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getInputs import WordIndex\n",
    "from formatScript import makeDialogue\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_dir = './scripts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_list = [script_dir + name for name in os.listdir(script_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = []\n",
    "unread = []\n",
    "for script in script_list:\n",
    "    try:\n",
    "        with open(script) as f:\n",
    "            dialogues.extend(makeDialogue(f.read()))\n",
    "    except:\n",
    "        unread.append(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201250"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(script_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1180"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(dialogue[0]) for dialogue in dialogues])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's filter longer lines for now and also those weirdly short ones that went under the radar\n",
    "def lenBetween(lines, minimum, maximum):\n",
    "    len_first = len(lines[0])\n",
    "    len_second = len(lines[1])\n",
    "    return (len_first >= minimum) and (len_first <= maximum) and (len_second >= minimum) and (len_second <= maximum)\n",
    "\n",
    "dialogues = [dialogue for dialogue in dialogues if lenBetween(dialogue, 3, 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107259"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to export this too\n",
    "def indexEmbed(dialogues):\n",
    "    language = WordIndex()\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        for line in dialogue:\n",
    "            language.fillCounts(line)\n",
    "\n",
    "    return language.getIndicies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = indexEmbed(dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe refactor this there has to be a simple way to reference random points off list even though it's tuples\n",
    "def chooseDialogues(dialogues, batch_size):\n",
    "    lines = len(dialogues)\n",
    "    chosen = [0 for i in range(batch_size)]\n",
    "    for i in range(batch_size):\n",
    "        chosen[i] = dialogues[np.random.randint(0, lines)]\n",
    "    return chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an embedding from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padLines(lines, token = 0):\n",
    "    return list(zip_longest(*lines, fillvalue=token))\n",
    "\n",
    "def prepareBatch(dialogues, token = 0):\n",
    "    inputs = [[language[word] for word in dialogue[0]] for dialogue in dialogues]\n",
    "    inputs.sort(key=lambda x: len(x), reverse=True)\n",
    "    lengths = torch.tensor([len(sentence) for sentence in inputs])\n",
    "    inputs = padLines(inputs)\n",
    "    inputs = torch.tensor(inputs)\n",
    "    outputs = [[language[word] for word in dialogue[1]] for dialogue in dialogues]\n",
    "    outputs = padLines(outputs)\n",
    "    outputs = torch.tensor(outputs)\n",
    "    mask = [[bool(word != token) for word in row] for row in outputs]\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    return inputs, lengths, outputs, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOS', 'BOS', 'BOS', 'BOS', 'BOS'),\n",
       " ('could', 'is', 'diego', 'diego', 'diego'),\n",
       " ('it', 'that', 'EOS', 'arent', 'take'),\n",
       " ('be', 'the', 0, 'you', 'fernando'),\n",
       " ('so', 'man', 0, 'going', 'to'),\n",
       " ('EOS', 'i', 0, 'to', 'the'),\n",
       " (0, 'knew', 0, 'kiss', 'dining'),\n",
       " (0, 'treasurer', 0, 'your', 'hall'),\n",
       " (0, 'sanchez', 0, 'brother', 'he'),\n",
       " (0, 'EOS', 0, 'EOS', 'must'),\n",
       " (0, 0, 0, 0, 'be'),\n",
       " (0, 0, 0, 0, 'hungry'),\n",
       " (0, 0, 0, 0, 'EOS')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(padLines([dialogue[0] for dialogue in dialogues[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tuple should be a step across every single line in the batch, so zip_longest fits the bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  1,  1,  1,  1],\n",
       "         [18, 10, 18,  3, 18],\n",
       "         [25, 11, 19,  4,  2],\n",
       "         [26, 12, 20,  5,  0],\n",
       "         [22, 13, 21,  6,  0],\n",
       "         [12, 14, 22,  2,  0],\n",
       "         [27, 15, 23,  0,  0],\n",
       "         [28, 16,  8,  0,  0],\n",
       "         [29, 17, 24,  0,  0],\n",
       "         [30,  2,  2,  0,  0],\n",
       "         [ 5,  0,  0,  0,  0],\n",
       "         [31,  0,  0,  0,  0],\n",
       "         [ 2,  0,  0,  0,  0]]),\n",
       " tensor([13, 10, 10,  6,  3]),\n",
       " tensor([[ 1,  1,  1,  1,  1],\n",
       "         [ 7,  7, 18, 18,  7],\n",
       "         [ 8,  8, 19, 25, 32],\n",
       "         [ 9,  9, 20, 26,  2],\n",
       "         [ 2,  2, 21, 22,  0],\n",
       "         [ 0,  0, 22, 12,  0],\n",
       "         [ 0,  0, 23, 27,  0],\n",
       "         [ 0,  0,  8, 28,  0],\n",
       "         [ 0,  0, 24, 29,  0],\n",
       "         [ 0,  0,  2, 30,  0],\n",
       "         [ 0,  0,  0,  5,  0],\n",
       "         [ 0,  0,  0, 31,  0],\n",
       "         [ 0,  0,  0,  2,  0]]),\n",
       " tensor([[1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0],\n",
       "         [0, 0, 1, 1, 0],\n",
       "         [0, 0, 1, 1, 0],\n",
       "         [0, 0, 1, 1, 0],\n",
       "         [0, 0, 1, 1, 0],\n",
       "         [0, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 1, 0]], dtype=torch.uint8))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepareBatch(dialogues[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    #as always, need to define forward pass of nn's in torch\n",
    "    def __init__(self, layers, nodes, embedding, dropout=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.nodes = nodes\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(nodes, nodes, layers, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "    def forward(self, inputs, lengths, h_in=None):\n",
    "        embedded = self.embedding(inputs)\n",
    "        transformed = nn.utils.rnn.pack_padded_sequence(embedded, lengths)\n",
    "        output, h_out = self.gru(transformed, h_in)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
    "        output = output[:,:,:self.nodes] + output[:,:,self.nodes:]\n",
    "        return output, h_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defined for a single step\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers, nodes, out_size, embedding, dropout=0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.nodes = nodes\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(nodes, nodes, layers, dropout=dropout)\n",
    "        self.out = nn.Linear(nodes, out_size)\n",
    "        \n",
    "    def forward(self, inputs, h_in):\n",
    "        embedded = self.embedding(inputs)\n",
    "        output, h_out = self.gru(embedded, h_in)\n",
    "        output = self.out(output)\n",
    "        output = torch.nn.functional.softmax(output, dim = 2)\n",
    "        return output, h_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 100\n",
    "layers = 2\n",
    "batch_size = 5\n",
    "num_words = len(language)\n",
    "embedding = nn.Embedding(num_words, nodes)\n",
    "encoder = Encoder(layers, nodes, embedding)\n",
    "decoder = Decoder(layers, nodes, num_words, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.0001\n",
    "e_optimizer = Adam(encoder.parameters(), lr=learn_rate)\n",
    "d_optimizer = Adam(decoder.parameters(), lr=10*learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_batch, in_lens, out_batch, out_mask = prepareBatch(dialogues[:batch_size])\n",
    "enc_outs, enc_hidden = encoder.forward(in_batch, in_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_ins = torch.ones(1, batch_size, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_outs, dec_hidden = decoder.forward(dec_ins, enc_hidden[:layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dec_outs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 28617])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.view(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = out_batch[1].view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.gather(a, 1, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000]], grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-10.2772, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(c).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 5])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also returns total number of items loss is computed over, used to find average loss\n",
    "def maskedLoss(result, target, mask):\n",
    "    n_terms = mask.sum().item()\n",
    "    probs = torch.gather(result, 1, target.view(-1,1))#gets probabilities for classes it should've predicted\n",
    "    loss = -torch.log(probs).masked_select(mask).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIter(encoder, decoder, e_optimizer, d_optimizer, dialogues, grad_max=100, print_now=False):\n",
    "    e_optimizer.zero_grad()\n",
    "    d_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    in_batch, in_lens, out_batch, out_mask = prepareBatch(dialogues)\n",
    "    enc_outs, enc_hidden = encoder.forward(in_batch, in_lens)\n",
    "    dec_ins = torch.ones(1, batch_size, dtype=torch.long)\n",
    "    dec_h_ins = enc_hidden[:layers]\n",
    "    for target, mask in zip(out_batch[1:,:], out_mask[1:, :]):\n",
    "        dec_outs, dec_hidden = decoder.forward(dec_ins, dec_h_ins)\n",
    "        loss += maskedLoss(dec_outs[0], target, mask)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(encoder.parameters(), grad_max)\n",
    "    nn.utils.clip_grad_norm_(decoder.parameters(), grad_max)\n",
    "    e_optimizer.step()\n",
    "    d_optimizer.step()\n",
    "    if print_now:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this one actually goes through an entire batch to decode a sentence\n",
    "#grabs most likely\n",
    "class GreedySearch(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearch, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        in_batch, in_lens, out_batch, out_mask = prepareBatch(batch)\n",
    "        enc_outs, enc_hidden = self.encoder(in_batch, in_lens)\n",
    "        dec_ins = torch.ones(1, in_batch.shape[1], dtype=torch.long)\n",
    "        dec_hidden = enc_hidden[:self.encoder.layers]\n",
    "        words = torch.ones(out_batch.shape[1]).long().unsqueeze(0)\n",
    "        scores = torch.ones(out_batch.shape[1]).unsqueeze(0)\n",
    "        for _ in range(in_batch.shape[0]-1):\n",
    "            dec_outs, dec_hidden = self.decoder.forward(dec_ins, dec_hidden[:self.decoder.layers])\n",
    "            score, dec_ins = torch.max(dec_outs, dim=2)\n",
    "            words = torch.cat((words, dec_ins))\n",
    "            scores = torch.cat((scores,score))\n",
    "        return words, scores, out_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.0871, grad_fn=<ThAddBackward>)\n",
      "tensor(80.9185, grad_fn=<ThAddBackward>)\n",
      "tensor(58.6604, grad_fn=<ThAddBackward>)\n",
      "tensor(56.4268, grad_fn=<ThAddBackward>)\n",
      "tensor(63.4824, grad_fn=<ThAddBackward>)\n",
      "tensor(68.6804, grad_fn=<ThAddBackward>)\n",
      "tensor(72.5847, grad_fn=<ThAddBackward>)\n",
      "tensor(60.0998, grad_fn=<ThAddBackward>)\n",
      "tensor(57.7197, grad_fn=<ThAddBackward>)\n",
      "tensor(64.1294, grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    chosen = chooseDialogues(dialogues, batch_size)\n",
    "    trainIter(encoder, decoder, e_optimizer, d_optimizer, chosen, print_now=(i%10==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
